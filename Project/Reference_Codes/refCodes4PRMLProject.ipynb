{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAxY2Z9kqhxj",
        "outputId": "c42213b5-9d84-43b6-9f13-b5ad3b5c728e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:02<00:00, 36.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2048, 1, 1])\n",
            "tensor([[[0.0288]],\n",
            "\n",
            "        [[0.1967]],\n",
            "\n",
            "        [[0.0153]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0000]],\n",
            "\n",
            "        [[0.0017]],\n",
            "\n",
            "        [[0.1831]]])\n"
          ]
        }
      ],
      "source": [
        "#This code can be used to extract pretrained CNN (ResNet) Features.\n",
        "#It takes Image Path and returns 2048-dimensional feature.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load pre-trained ResNet-50 model\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "# Remove the last fully connected layer\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "# Set the model to evaluation mode\n",
        "resnet.eval()\n",
        "\n",
        "# Define a function to extract features from an image\n",
        "def extract_features(image_path, model):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = preprocess(image)\n",
        "    # Add batch dimension\n",
        "    image = image.unsqueeze(0)\n",
        "    # Extract features\n",
        "    with torch.no_grad():\n",
        "        features = model(image)\n",
        "    # Remove the batch dimension\n",
        "    features = features.squeeze(0)\n",
        "    return features\n",
        "\n",
        "# Example usage\n",
        "image_path = 'example_image.png'  # Replace 'example_image.jpg' with your image path\n",
        "features = extract_features(image_path, resnet)\n",
        "print(features.shape)  # Output: torch.Size([2048])\n",
        "print(features)\n",
        "\n",
        "# Save the extracted features\n",
        "torch.save(features, 'extracted_features.pt')  # Save the features to a file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This code can be used to extract Histogram of Gradient (HoG) Features.\n",
        "#It takes Image Path and returns HoG feature.\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def compute_hog(img):\n",
        "  #resizing image\n",
        "  resized_img = resize(img, (128*4, 64*4))\n",
        "  #creating hog features\n",
        "  fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
        "                    cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
        "  return fd, hog_image\n",
        "\n",
        "#reading the image\n",
        "img = imread('example_image.png')\n",
        "Hog_feature, hog_image=compute_hog(img)\n",
        "print(Hog_feature.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuxjYeXzrW1x",
        "outputId": "250632e5-96e5-4967-e4c8-819877d019f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70308,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This code can be used to extract Histogram of Gradient (HoG) Features.\n",
        "#It takes Image Path and returns HoG feature.\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def get_pixel(img, center, x, y):\n",
        "    new_value = 0\n",
        "    try:\n",
        "        if img[x][y] >= center:\n",
        "            new_value = 1\n",
        "    except:\n",
        "        pass\n",
        "    return new_value\n",
        "\n",
        "def lbp_calculated_pixel(img, x, y):\n",
        "    center = img[x][y]\n",
        "    val_ar = []\n",
        "    val_ar.append(get_pixel(img, center, x-1, y+1))     # top_right\n",
        "    val_ar.append(get_pixel(img, center, x, y+1))       # right\n",
        "    val_ar.append(get_pixel(img, center, x+1, y+1))     # bottom_right\n",
        "    val_ar.append(get_pixel(img, center, x+1, y))       # bottom\n",
        "    val_ar.append(get_pixel(img, center, x+1, y-1))     # bottom_left\n",
        "    val_ar.append(get_pixel(img, center, x, y-1))       # left\n",
        "    val_ar.append(get_pixel(img, center, x-1, y-1))     # top_left\n",
        "    val_ar.append(get_pixel(img, center, x-1, y))       # top\n",
        "\n",
        "    power_val = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "    val = 0\n",
        "    for i in range(len(val_ar)):\n",
        "        val += val_ar[i] * power_val[i]\n",
        "    return val\n",
        "\n",
        "\n",
        "def calcLBP(img):\n",
        "    height, width, channel = img.shape\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img_lbp = np.zeros((height, width,3), np.uint8)\n",
        "    for i in range(0, height):\n",
        "        for j in range(0, width):\n",
        "             img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n",
        "    hist_lbp = cv2.calcHist([img_lbp], [0], None, [256], [0, 256])\n",
        "    return(hist_lbp)\n",
        "\n",
        "\n",
        "image_file = 'example_img.png'\n",
        "img = cv2.imread(image_file)\n",
        "lbpFeature=calcLBP(img)\n",
        "print(lbpFeature)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "3ovy7eJlb7wV",
        "outputId": "1b433181-7c6a-430b-9e66-7bd17b069891"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-748dc9e08d8b>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'example_img.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mlbpFeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalcLBP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbpFeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-748dc9e08d8b>\u001b[0m in \u001b[0;36mcalcLBP\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalcLBP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mimg_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This code takes sentence and gives 768-dimensional BERT embedding.\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import gensim.downloader as api\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "def get_bert_embedding(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "    return embedding\n",
        "\n",
        "# Example usage\n",
        "sentence = \"This is sample sentence.\"\n",
        "bert_embedding = get_bert_embedding(sentence)\n",
        "print(\"BERT Embedding:\", bert_embedding.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2QMwQ75dxpy",
        "outputId": "8f3daa93-7614-49b5-9c0c-9dbc01731820"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Embedding: (768,)\n"
          ]
        }
      ]
    }
  ]
}